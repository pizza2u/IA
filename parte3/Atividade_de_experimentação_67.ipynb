{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9P_7dtJuGsk"
      },
      "source": [
        "# **Inteligência Artificial aplicada à Visão Computacional**\n",
        "\n",
        "**Capítulo 6: Visão Computacional aplicada ao reconhecimento de emoções**\n",
        "\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "\n",
        "Todos os direitos reservados à Facti, 2024\n",
        "<p>\n",
        "\n",
        "[www.qualifacti.com.br](https://)\n",
        "\n",
        "---\n",
        "\n",
        "<p align=\"justify\">\n",
        "É importante esclarecer que estas atividades não compõem a avaliação e não haverá correção formal por parte dos instrutores; o objetivo é a autoaprendizagem e prática.\n",
        "<p>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F-hvs7WrjLy"
      },
      "source": [
        "# **Atividade de experimentação 67**\n",
        "\n",
        "Testando o modelo do detector de emoções pela webcam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz8aXNC5tOyC"
      },
      "source": [
        "## Importando as bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "NpYBsKx6raIF",
        "outputId": "a1cb17e0-9334-4e85-a897-558cb2d706b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.15.0'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab.patches import cv2_imshow\n",
        "import zipfile\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "tensorflow.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcVgKMKIrGGS"
      },
      "source": [
        "## Conectando com o Drive e acessando os arquivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3POz78BCrC9P",
        "outputId": "a646b023-a57a-4e1a-d75a-d921ec7abc97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<bound method ZipFile.close of <zipfile.ZipFile filename='/content/gdrive/MyDrive/Material_complementar_reconhecimento_emocoes.zip' mode='r'>>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Conectando o Colab ao Google Drive\n",
        "# Realize o dowload da pasta Material_complementar_reconhecimento_emocoes.zip do Google Sala de Aula e transfira-a para o seu Google Drive\n",
        "# Localize o caminho da pasta no menu Arquivos, no menu lateral esquerdo\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "path = \"/content/gdrive/MyDrive/Material_complementar_reconhecimento_emocoes.zip\"\n",
        "zip_object = zipfile.ZipFile(file=path, mode=\"r\")\n",
        "zip_object.extractall(\"./\")\n",
        "zip_object.close"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECLyswiCta1B"
      },
      "source": [
        "## Testando a foto capturada da webcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPiuEyB4twND"
      },
      "outputs": [],
      "source": [
        "# Script para visualização de vídeo pela webcam\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "VIDEO_HTML = \"\"\"\n",
        "<video autoplay\n",
        " width=%d height=%d style='cursor: pointer;'></video>\n",
        "<script>\n",
        "\n",
        "var video = document.querySelector('video')\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({ video: true })\n",
        "  .then(stream=> video.srcObject = stream)\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "  video.onclick = ()=>{\n",
        "    var canvas = document.createElement('canvas')\n",
        "    var [w,h] = [video.offsetWidth, video.offsetHeight]\n",
        "    canvas.width = w\n",
        "    canvas.height = h\n",
        "    canvas.getContext('2d')\n",
        "          .drawImage(video, 0, 0, w, h)\n",
        "    video.srcObject.getVideoTracks()[0].stop()\n",
        "    video.replaceWith(canvas)\n",
        "    resolve(canvas.toDataURL('image/jpeg', %f))\n",
        "  }\n",
        "})\n",
        "</script>\n",
        "\"\"\"\n",
        "def tirar_foto(filename='photo.jpg', quality=2, size=(400,300)):\n",
        "  display(HTML(VIDEO_HTML % (size[0],size[1],quality)))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  f = io.BytesIO(binary)\n",
        "  return np.asarray(Image.open(f))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxLMDPCGuIvk"
      },
      "source": [
        "**Capturando a foto**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roHOUgizt-XW"
      },
      "outputs": [],
      "source": [
        "# Clique na imagem da webcam para tirar uma foto\n",
        "imagem = tirar_foto()\n",
        "# Inverte a ordem dos canais (utilizar caso a imagem capturada fique com cores invertidas)\n",
        "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
        "cv2_imshow(imagem)\n",
        "cv2.imwrite(\"testecaptura.jpg\",imagem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKTI2_tArjil"
      },
      "outputs": [],
      "source": [
        "# Utilize um haarcasdade pré treinado para o reconhecimento facial\n",
        "# Utilize um modelo pré treinado para o reconhecimento das emoções\n",
        "cascade_faces = '/content/haarcascade_frontalface_default.xml'\n",
        "caminho_modelo = '/content/modelo_01_expressoes.h5'\n",
        "face_detection = cv2.CascadeClassifier(cascade_faces)\n",
        "classificador_emocoes = load_model(caminho_modelo, compile=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBhzNFgMuNH4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Carrega o modelo\n",
        "face_detection = cv2.CascadeClassifier(cascade_faces)\n",
        "classificador_emocoes = load_model(caminho_modelo, compile=False)\n",
        "# Classifica cada uma das categorias das expressões\n",
        "expressoes = [\"Raiva\", \"Nojo\", \"Medo\", \"Feliz\", \"Triste\", \"Surpreso\", \"Neutro\"]\n",
        "\n",
        "original = imagem.copy()\n",
        "faces = face_detection.detectMultiScale(original,scaleFactor=1.1,minNeighbors=3,minSize=(20,20))\n",
        "cinza = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "if len(faces) > 0:\n",
        "    for (fX, fY, fW, fH) in faces:\n",
        "      roi = cinza[fY:fY + fH, fX:fX + fW]\n",
        "      roi = cv2.resize(roi, (48, 48))\n",
        "      roi = roi.astype(\"float\") / 255.0\n",
        "      roi = img_to_array(roi)\n",
        "      roi = np.expand_dims(roi, axis=0)\n",
        "      preds = classificador_emocoes.predict(roi)[0]\n",
        "      print(preds)\n",
        "      emotion_probability = np.max(preds)\n",
        "      label = expressoes[preds.argmax()]\n",
        "      cv2.putText(original, label, (fX, fY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "      cv2.rectangle(original, (fX, fY), (fX + fW, fY + fH),(0, 0, 255), 2)\n",
        "else:\n",
        "    print('Nenhuma face detectada')\n",
        "\n",
        "\n",
        "cv2_imshow(original)\n",
        "\n",
        "probabilidades = np.ones((250, 300, 3), dtype=\"uint8\") * 255\n",
        "# Mostra gráfico apenas se detectou uma face\n",
        "if len(faces) == 2:\n",
        "  for (i, (emotion, prob)) in enumerate(zip(expressoes, preds)):\n",
        "      # Nome das emoções\n",
        "      text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
        "      w = int(prob * 300)\n",
        "      cv2.rectangle(probabilidades, (7, (i * 35) + 5),\n",
        "      (w, (i * 35) + 35), (200, 250, 20), -1)\n",
        "      cv2.putText(probabilidades, text, (10, (i * 35) + 23),\n",
        "      cv2.FONT_HERSHEY_SIMPLEX, 0.45,\n",
        "      (0, 0, 0), 1, cv2.LINE_AA)\n",
        "\n",
        "  cv2_imshow(probabilidades)\n",
        "\n",
        "cv2.imwrite(\"captura.jpg\",original)\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
